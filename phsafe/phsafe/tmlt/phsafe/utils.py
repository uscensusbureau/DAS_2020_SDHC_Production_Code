"""Miscellaneous tools for PHSafe."""

# Copyright 2024 Tumult Labs
# 
#    Licensed under the Apache License, Version 2.0 (the "License");
#    you may not use this file except in compliance with the License.
#    You may obtain a copy of the License at
# 
#        http://www.apache.org/licenses/LICENSE-2.0
# 
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS,
#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#    See the License for the specific language governing permissions and
#    limitations under the License.
import glob
import itertools
import json
import logging
import os
import pkgutil
from io import StringIO
from math import exp, isclose
from pathlib import Path
from typing import Any, Callable, Dict, List, Mapping, Tuple
from urllib.parse import urlparse

import boto3
import pandas as pd
from pyspark.sql import DataFrame, Row
from pyspark.sql import functions as sf
from smart_open import open  # pylint: disable=redefined-builtin

from tmlt.common.configuration import Config
from tmlt.common.io_helpers import multi_read_csv
from tmlt.common.validation import validate_file
from tmlt.phsafe import TABULATION_OUTPUT_COLUMNS, TABULATIONS_KEY
from tmlt.phsafe.paths import (
    ALT_INPUT_CONFIG_DIR,
    ALT_OUTPUT_CONFIG_DIR,
    CONFIG_FILES_PHSAFE,
    INPUT_CONFIG_DIR,
    OUTPUT_CONFIG_DIR,
    VALIDATION_CONFIG_FILES,
)

DATA_CELL_MAP = {
    "PH1_num": "PH1_NUM",
    "PH1_denom": "PH1_DENOM",
    "PH2": "PH2",
    "PH3": "PH3",
    "PH4": "PH4",
    "PH5_num": "PH5_NUM",
    "PH5_denom": "PH5_DENOM",
    "PH6": "PH6",
    "PH7": "PH7",
    "PH8_num": "PH8_NUM",
    "PH8_denom": "PH8_DENOM",
}


def get_augmented_df(
    name: str, noisy_path: str, ground_truth_path: str
) -> pd.DataFrame:
    """Return a dataframe containing both noisy and ground_truth counts.

    Args:
        name: Name of the file. Either "PH1_denom", "PH1_num", "PH2", "PH3", "PH4",
            "PH5_num", "PH5_denom", "PH6", "PH7", "PH8_num", or "PH8_denom".
        noisy_path: Directory containing the counts or sums generated by a
            differentially private mechanism for PHSafe.
        ground_truth_path: Directory containing the ground truth counts or sums for
            PHSafe.

    Returns:
        A dataframe containing all of the expected columns from `Appendix A`,
        minus "COUNT"/"SUM". The dataframe is augmented with the following
        columns:

        * NOISY: The counts from the mechanism, as integers.
        * GROUND_TRUTH: The ground truth counts, as integers.
    """
    noisy_df = multi_read_csv(os.path.join(noisy_path, name), sep="|", dtype=str)
    ground_truth_df = multi_read_csv(
        os.path.join(ground_truth_path, name), sep="|", dtype=str
    )
    if "SUM" in noisy_df.columns:
        col = "SUM"
    else:
        if "COUNT" not in noisy_df.columns:
            raise ValueError("Noisy data must contain either a SUM or COUNT column.")
        col = "COUNT"
    noisy_df = noisy_df.rename(columns={col: "NOISY"})
    ground_truth_df = ground_truth_df.rename(columns={col: "GROUND_TRUTH"})
    df = pd.merge(noisy_df, ground_truth_df, how="left").fillna(0)
    df["NOISY"] = df["NOISY"].astype(int)
    df["GROUND_TRUTH"] = df["GROUND_TRUTH"].astype(int)
    return df


def get_augmented_df_from_spark(
    noisy_df: DataFrame, ground_truth_df: DataFrame
) -> DataFrame:
    """Joins the noisy and ground truth results into one dataframe in spark.

    Args:
        noisy_df: The noisy differentially private count or sum.
        ground_truth_df: The ground truth count or sum.

    Returns:
        A dataframe containing all of the expected columns from `Appendix A`,
        minus "COUNT"/"SUM". The dataframe is augmented with the following
        columns:

        * NOISY: The counts from the mechanism, as integers.
        * GROUND_TRUTH: The ground truth counts, as integers.
    """
    if "SUM" in noisy_df.columns:
        col = "SUM"
    else:
        if "COUNT" not in noisy_df.columns:
            raise ValueError("Noisy data must contain either a SUM or COUNT column.")
        col = "COUNT"
    noisy_df = noisy_df.withColumnRenamed(col, "NOISY")
    ground_truth_df = ground_truth_df.withColumnRenamed(col, "GROUND_TRUTH")
    join_columns = list(set(noisy_df.columns).intersection(ground_truth_df.columns))
    return noisy_df.join(ground_truth_df, on=join_columns, how="left").fillna(0)


def validate_directory_single_config(
    input_dir: str, config_file: str, **kwargs: Any
) -> bool:
    """Validate all files in a directory against a single config using validate_file.

    The passed directory should contain only files that should be validated.

    Args:
        input_dir: The directory containing the csv files to validate.
        config_file: The filename of the config to validate against.
        kwargs: Keyword arguments to pass to validate_file.
    """
    filenames = glob.glob(f"{input_dir}/*.csv")
    config = Config.load_json(config_file)
    okay = True
    for filename in filenames:
        if os.stat(filename).st_size > 0:
            okay &= validate_file(filename, config, **kwargs)
    return okay


def update_config_file(config_file: str, settings: Mapping) -> None:
    """Update config.json file by passing a new dictionary of setting.

    Args:
        config_file: The path to json config file.
        settings: settings in dictionary for config file.
    """
    with open(config_file, "r") as conf:
        data = json.load(conf)
    for arg in settings:
        data[arg] = settings[arg]
    with open(config_file, "w") as conf:
        json.dump(data, conf)


def setup_input_config_dir():
    """Copy resources/config/input contents to temp directory ALT_INPUT_CONFIG_DIR.

    NOTE: This setup is required to ensure zip-compatibility of PHSafe.
    In particular, configs in resources directory of PHSafe can not read
    when PHSafe is distributed (and invoked) as zip archive (See Issue #331)
    """
    os.makedirs(ALT_INPUT_CONFIG_DIR, exist_ok=True)
    for json_filename in CONFIG_FILES_PHSAFE:
        json_file = Path(os.path.join(ALT_INPUT_CONFIG_DIR, json_filename))
        json_file.touch(exist_ok=True)
        json_file.write_bytes(
            pkgutil.get_data(
                "tmlt.phsafe", os.path.join(INPUT_CONFIG_DIR, json_filename)
            )
        )


def setup_output_config_dir():
    """Copy resources/config/output contents to temp directory ALT_OUTPUT_CONFIG_DIR.

    NOTE: This setup is required to ensure zip-compatibility of PHSafe.
    In particular, configs in resources directory of PHSafe can not read
    when PHSafe is distributed (and invoked) as zip archive (See Issue #331)
    """
    os.makedirs(ALT_OUTPUT_CONFIG_DIR, exist_ok=True)
    for cfg_file in TABULATION_OUTPUT_COLUMNS:
        json_filename = cfg_file + ".json"
        json_file = Path(os.path.join(ALT_OUTPUT_CONFIG_DIR, json_filename))
        json_file.touch(exist_ok=True)
        json_file.write_bytes(
            pkgutil.get_data(
                "tmlt.phsafe", os.path.join(OUTPUT_CONFIG_DIR, json_filename)
            )
        )
    for cfg_file in VALIDATION_CONFIG_FILES:
        json_file = Path(os.path.join(ALT_OUTPUT_CONFIG_DIR, cfg_file))
        json_file.touch(exist_ok=True)
        json_file.write_bytes(
            pkgutil.get_data("tmlt.phsafe", os.path.join(OUTPUT_CONFIG_DIR, cfg_file))
        )


def get_config_privacy_budget_dict(
    privacy_budget: float,
) -> Dict[str, Dict[str, float]]:
    """Builds and returns config's privacy budget dictionary for all tabulations.

    Args:
        privacy_budget: Budget for each population group in all tabulations.
    """
    tabulation_geo_iteration_budget_dict = {}
    for tabulation in TABULATIONS_KEY:
        if tabulation in ["PH2", "PH6"]:
            tabulation_geo_iteration_budget_dict[tabulation] = {
                geo_iteration: privacy_budget for geo_iteration in ["usa_*", "state_*"]
            }
        else:
            tabulation_geo_iteration_budget_dict[tabulation] = {
                geo_iteration: privacy_budget
                for geo_iteration in [
                    "usa_A-G",
                    "usa_H,I",
                    "usa_*",
                    "state_A-G",
                    "state_H,I",
                    "state_*",
                ]
            }
    return tabulation_geo_iteration_budget_dict


def _geometric_variance(noise_parameter: float) -> float:
    """Calculates the variance of a double-sided geometric distribution."""
    if noise_parameter == 0:
        return 0
    return 2 * exp(-1 / noise_parameter) / ((1 - exp(-1 / noise_parameter)) ** 2)


def _dp_standard(tau: int, budget: float) -> float:
    """One of the variance formulas for the puredp privacy definition.

    Variance is approximated by Laplace distribution.
    """
    return _geometric_variance((2 * tau + 2) / budget)


def _dp_denom(budget: float) -> float:
    """One of the variance formulas for the puredp privacy definition.

    Variance is approximated by Laplace distribution.
    """
    return _geometric_variance(2 / budget)


def _dp_double(tau: int, budget: float) -> float:
    """One of the variance formulas for the puredp privacy definition.

    Variance is approximated by Laplace distribution.
    """
    return 2 * _dp_standard(tau, budget)


def _zc_standard(tau: int, budget: float) -> float:
    """One of the variance formulas for the zcdp privacy definition.

    Variance is approximated by continuous Guassian distribution.
    """
    return ((2 * tau + 2) ** 2) / (budget * 2)


def _zc_denom(budget: float) -> float:
    """One of the variance formulas for the zcdp privacy definition.

    Variance is approximated by continuous Guassian distribution.
    """
    return 2 / budget


def _zc_double(tau: int, budget: float) -> float:
    """One of the variance formulas for the zcdp privacy definition.

    Variance is approximated by continuous Guassian distribution.
    """
    return ((2 * tau + 2) ** 2) / budget


def validate_output_variance(
    privacy_budget: Mapping[str, Mapping[str, float]],
    taus: Mapping[str, int],
    data: Mapping[str, DataFrame],
    validation_config: Mapping[str, Mapping[str, str]],
) -> bool:
    """Validates the variance produced from all tabulations.

    Args:
        privacy_budget: Privacy Budget for each tabulation.
        taus: Tau values used for each applicable tabulation.
        data: The Spark DataFrame containing all the tabulation data.
        validation_config: The validation configuration file.

    Raises:
        RuntimeError: When tau values are not given.
    """
    # Variance Calculation Formulas
    formulas: Mapping[str, Callable[..., float]] = {
        "dp_standard": _dp_standard,
        "dp_denom": _dp_denom,
        "dp_double": _dp_double,
        "zc_standard": _zc_standard,
        "zc_denom": _zc_denom,
        "zc_double": _zc_double,
    }

    logger = logging.getLogger(__name__)
    logger.info("Validating Tabulations")

    okay = True
    for tabulation, df in data.items():
        if df is None:
            continue
        budget_source = validation_config[tabulation]["budget"]
        tau = taus[budget_source] if budget_source in taus else 0
        okay = validate_tabulation_variance(
            tabulation_data=df,
            privacy_budget=privacy_budget,
            budget_source=budget_source,
            tabulation=tabulation,
            tabulation_config=validation_config[tabulation],
            formulas=formulas,
            tau=tau,
        )
        if not okay:
            return False
    return True


def _get_tabulation_dataframe(path: str, is_s3: bool) -> pd.DataFrame:
    """Returns tabulation dataframes.

    Given a csv path, reads either from s3 or from local and return the tabulation data.

    Args:
        path: Path to the csv, which can either be local or on aws s3
        is_s3: If the path is an s3 path or a local path.

    Return:
        Returns a pandas dataframe containing the tabulation data.
    """
    if not is_s3:
        return pd.readcsv(path, sep="|")
    s3 = boto3.client("s3")
    url = urlparse(path)
    s3_object = s3.get_object(Bucket=url.netloc, Key=url.path.lstrip("/"))
    csv_data = s3_object["Body"].read().decode("utf-8")
    return pd.read_csv(StringIO(csv_data), sep="|")


def _get_tabulation_csv_files(output_path: str, tabulation: str) -> List[str]:
    """Returns all csv files in the tabulation's directory.

    Given an output path and a specific tabulation, the function returns all csv files
        in the tabulation's directory. This version works for s3 paths.

    Args:
        output_path: Absolute path to the tabulations output directory.
        tabulation: Specific tabulation to retrieve from.

    Returns:
        A list of absolute directories pointing to all csv files under the
        tabulation.
    """
    cwd = os.path.join(output_path, tabulation)
    files: List[str] = []
    if not os.path.exists(cwd):
        return files
    for output in os.listdir(cwd):
        if not output.endswith(".csv"):
            continue
        files.append(os.path.join(cwd, output))
    return files


def _get_s3_tabulation_csv_files(output_path: str, tabulation: str) -> List[str]:
    """Returns all csv files in the tabulation's directory.

    Given an output path and a specific tabulation, the function returns all csv files
        in the tabulation's directory. This version works for s3 paths.

    Args:
        output_path: Absolute path to the tabulations output directory.
        tabulation: Specific tabulation to retrieve from.

    Returns:
        A list of absolute directories pointing to all csv files under the
        tabulation.
    """
    files: List[str] = []
    s3 = boto3.client("s3")
    url = urlparse(output_path)
    s3_objects = s3.list_objects_v2(
        # s3n is required to navigate s3, s3 will be used later for retrieval
        Bucket=url.netloc.replace("s3", "s3n", 1),
        Prefix=f"{url.path.lstrip('/')}{tabulation}",
    )
    for s3_object in s3_objects["Contents"]:
        if not s3_object["Key"].endswith(".csv"):
            continue
        files.append(f"{url.scheme}://{url.netloc}/{s3_object['Key']}")
    return files


def _validate_variance_validation_configs(
    validation_config: Mapping[str, Mapping[str, str]], taus: Mapping[str, int]
):
    """Validates the three configs used in validating the variance of the tabulations.

    Args:
        validation_config: The configuration that contains the function mappings
            and budget sources.
        taus: The tau values associated with each tabulation.

    Raises:
        RuntimeError: When tau values are not given for the correct tabulation
            or formula.
    """
    for tabulation, tabulation_config in validation_config.items():
        for _, formula in tabulation_config.items():
            budget_source = tabulation_config["budget"]
            # denom formulas do not require tau, but the rest do
            if not formula.endswith("denom"):
                if not budget_source in taus:
                    raise RuntimeError(
                        f"Configuration validation error on {tabulation}"
                        "Tau was not provided"
                    )
                if taus[budget_source] == 0:
                    raise RuntimeError(
                        f"Configuration validation error on {tabulation}"
                        "Tau was set to 0"
                    )


def validate_tabulation_variance(
    tabulation_data: DataFrame,
    privacy_budget: Mapping[str, Mapping[str, float]],
    budget_source: str,
    tabulation: str,
    tabulation_config: Mapping[str, str],
    formulas: Mapping[str, Callable[..., float]],
    tau: int,
) -> bool:
    """Validates the variance produced from a specific tabulation.

    Args:
        tabulation_data: The full dataframe of the tabulation.
        privacy_budget: Privacy Budget for each tabulation.
        budget_source: Which tabulation it should use for its budget.
        tabulation: The tabulation name.
        tabulation_config: Validation configuration specific to tabulation.
        formulas: Applicable formulas.
        tau: Tau specific to this tabulation.

    Raises:
        RuntimeError: When tau values are not given.
    """
    # Precompute all possible combinations
    REGION_CODES = ["USA", "STATE"]
    ITERATION_CODES = ["A-G", "H,I", "*"]

    logger = logging.getLogger(__name__)
    logger.info("Validating %s", tabulation)
    variance_mappings: Dict[Tuple[str, str], Dict[str, float]] = {}
    for data_cell, formula in tabulation_config.items():
        if data_cell == "budget":
            continue
        for region, iteration in itertools.product(REGION_CODES, ITERATION_CODES):
            region_iteration = f"{region.lower()}_{iteration}"
            if (region_iteration) not in privacy_budget[budget_source]:
                continue
            budget = privacy_budget[budget_source][region_iteration]
            if budget == 0:
                continue
            if formula.endswith("denom"):
                expected_variance = formulas[formula](budget)
            else:
                expected_variance = formulas[formula](tau, budget)
            if (region, iteration) not in variance_mappings:
                variance_mappings[(region, iteration)] = {}
            variance_mappings[(region, iteration)][data_cell] = expected_variance

    def append_row(row: Row) -> Row:
        dataCell = DATA_CELL_MAP[tabulation]
        cell = str(row[f"{dataCell}_DATA_CELL"])
        region = row["REGION_TYPE"]
        iteration_code = row["ITERATION_CODE"] if "ITERATION_CODE" in row else "*"
        iteration = _group_iterations(iteration_code)
        cell_mapping = variance_mappings[(region, iteration)]
        if cell not in cell_mapping:
            expected_variance = cell_mapping["*"]
        else:
            expected_variance = cell_mapping[cell]
        new_row = row.asDict()
        new_row["EXPECTED_VARIANCE"] = expected_variance
        new_row["AS_EXPECTED"] = isclose(
            float(row["VARIANCE"]), expected_variance, rel_tol=1e-6
        )
        return Row(**new_row)

    tabulation_data = tabulation_data.rdd.map(append_row).toDF()
    tabulation_data = tabulation_data.filter(
        ~sf.col("AS_EXPECTED")  # pylint: disable=no-member
    )
    errors = tabulation_data.count()
    if errors > 0:
        # We print out only 10 errors, lest the logs overflow
        rows = tabulation_data.take(10)
        for row in rows:
            logger.error(
                "Expected variance: %.5f | Actual variance: %.5f",
                row["EXPECTED_VARIANCE"],
                row["VARIANCE"],
            )
        logger.error("%s failed variance validation.", tabulation)
        logger.error("%i rows failed schema validation.", errors)
        return False
    logger.info("%s passed variance validation.", tabulation)
    return True


def _group_iterations(iteration: str) -> str:
    """Groups iterations into their respective buckets A-G, H,I, and *.

    Args:
        iteration: The iteration level.
    """
    if "A" <= iteration <= "G":
        return "A-G"
    if iteration in {"H", "I"}:
        return "H,I"
    return "*"
